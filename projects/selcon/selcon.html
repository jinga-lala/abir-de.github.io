
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <!-- MathJax configuration must come before MathJAX is loaded even if async -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        extensions: [ "color.js" ]
      }
    });
  </script>

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG" async></script>



  <title>
    
      Hyde &middot; A Jekyll theme
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="public/css/poole_old.css">
  <link rel="stylesheet" href="public/css/syntax.css">
  <link rel="stylesheet" href="public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>
</head>


  <body class="theme-base-08">
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
         <p class="small-caps">SelCon</p>
      </h1>
  

      <p class="lead"> It is a data subset selection algorithm which ensures efficient training without
    significant accuracy drop.</p>


 
    </div>

    <nav class="sidebar-nav">


      
      <a class="sidebar-nav-item" href="#">Paper</a>
      <a class="sidebar-nav-item" href="#">Code and data</a>
		<a class="sidebar-nav-item" href="#cnt">Contacts</a>
      
      <span class="sidebar-nav-item">Accepted @ICML 2021</span>
    </nav>

    <!-- p>Accepted @ICML 2021</p -->
  </div>
</div>

  
    <div class="content container">
      <div class="posts">
  
  <div class="post">
  
 	<p></p>
   <p><img src="public/css/selcon-style-v2.png" width=30% alt="placeholder" vspace="-30" title="Large example image" style="float:right">
    <h1 class="post-title">
   <a>     <p class="small-caps">SelCon</p> </a>
    </h1>


<!-- h3>Introduction</h3-->

<p>  One of the many ways towards efficient and cost-effective machine learning is 
to suitably select of a subset of training instances, so that the training
algorithm can be run in an environment with limited
resources. In our work, we design SELCON,
a new subset selection method for regression problem, 
which leads to an efficient training without significant loss of accuracy. 
 </p>
<h3>Salient features</h3>
<ul>
<li> We cast our problem as a joint optimization problem which aims to select the
training subset as well as the model parameters subject to a set of constraints
 ensuring that the error on validation set remains below an acceptable level. </li>
<li>We show that solving the above problem is equivalent
 to minimizing a monotone and approximate submodular function.</li>
<li> We design an approximation algorithm to solve the problem. Moreover, we also show an approximation guarantee
when the trained model parameters are imperfect estimates.</li>
</ul>

<h3> Problem setup </h3>

<p> Given the training data \( \{\pmb{x}_i,y_i\}_{\in\mathcal{D}} \), a validation set 
\( \{\pmb{x}_j,y_j\}_{j\in\mathcal{V}} \) along with its partitions
 \( \mathcal{V}=V_1 \cup V_2 ... \cup V_Q\), we learn to select \(\mathcal{S}\subset \mathcal{D}\)
and the parameters as follows:</p>
<p>
$$ \underset{\pmb{w}, \mathcal{S} \subset \mathcal{D}, \{\xi_q \ge 0\}}{\text{minimize}} \quad
\sum_{i\in \mathcal{S}}[\lambda ||\pmb{w}||^2 + (y_i-\pmb{w}^\top \pmb{x}_i)^2 ]+ C\sum_{q =1} ^Q \xi_q\\ 
 \text{subject to,  }\quad \frac{1}{|V_q|} \sum_{j\in V_q}(y_j-\pmb{w}^\top \pmb{x}_j)^2 \le \delta+\xi_q
 \ \forall\, q\in [Q], \\
  |\mathcal{S}|=k,\qquad\qquad \qquad\qquad $$
</p>
where \(\delta\) indicates the acceptable level of error in validation set and \(\{\xi_q\}\) are the slack variables
indicating the margin of error violation.
We convert them into the dual optimization problem as follows:
$$ \underset{\mathcal{S}}{\text{minimize}}\quad \underbrace{ \underset{\pmb{0}\le \pmb{\mu} \le C \pmb{1}}{\text{maximize}}
 \ \underset{\pmb{w}}{\text{minimize}} \quad F(\pmb{w},\pmb{\mu},\mathcal{S})}_{f(\mathcal{S})} $$
$$\text{where, }\ F(\pmb{w},\pmb{\mu},\mathcal{S}) =   \sum_{i\in\mathcal{S}}  
[ \lambda||{\pmb{w}}||^2 +  (y_i-h_{\pmb{w}}(\pmb{x} _i))^2] \\
  \qquad\qquad\qquad   \qquad\qquad\qquad +\sum_{q\in[Q]}  \hspace{-1mm} \mu_q  \hspace{-1mm}
  \left[ \frac{\sum_{j\in V_q}(y_j-h_{\pmb{w}}(\pmb{w}_j))^2}{|V_q|}  -  \delta\right]$$

<h3> Characterizations of \(f(\mathcal{S})\) </h3>
We show that \(f(\mathcal{S})\) is monotone, approximately submodular and
has finite curvature. More specifically, we have:
<ol>
<li>\(f(\mathcal{S}\cup a)-f(\mathcal{S}) \ge 0 \)  for all \(a \not \in \mathcal{S}\) </li>
<li>\(f(\mathcal{S})\) is \(\alpha\)-submodular with
$$\alpha \ge 1-\frac{32(1+CQ)^2 \max_i ||\pmb{x}_i y_i||}{ \lambda \min_i |y_i|} $$
 </li>
 <li>  \(f(\mathcal{S})\) has curvature \(\kappa\) with
$$\kappa \le 1-\frac{\min_i y_i ^2}{(1+CQ)} $$ 
  </li>
</ol>
 
<h3>Approximation  Algorithm</h3>

<p> Given a current estimate of \(\mathcal{S}\), \our 
computes modular approximation components \(\alpha f(i | \mathcal{S} \backslash i)\) for \(i\in \mathcal{S}\) and  \(f(i | \emptyset)/\alpha\) 
for \(i\not\in  \mathcal{S} \). The algorithm next picks the \(k\) smallest  values to minimize \( m \) and update  \(\mathcal{S}\). 
</p>

<div class="highlight"><pre><code class="language-js" data-lang="js"> 
<span class="c1">// Simultaneous computation of model parameters and S</span>
  <span class="p"> for epoch in range(L):</span> 
  <span class="p">	w, mu, f_S = train(S) </span> 
  <span class="p">	for j in range(S.size[0]):</span> 
  <span class="p">		Sj =  torch.cat([S[0:j], S[j+1:]])</span> 
  <span class="p">		_,_,f_Sj = train(Sj) </span> 
<span class="p">		m[i] = alpha *(f_S - f_Sj) </span>
  <span class="p">	for j in range(S_complement.size[0]):</span> 
  <span class="p">		m[j] =  f[j] / alpha </span> 
  <span class="p">	S, S_complement = split(m,k) </span> 
 </code></pre></div>

Our algorithm provides approximation bound as follows.

$$    f({\mathcal{S}}) \leq \frac{k}{\alpha (1 + (k-1)(1 - \kappa)\alpha)} f(\mathcal{S}^*)$$


<h3>Results</h3>
Our experiments show that our method
trades off accuracy and efficiency more effectively than the current state-of-the-art. As an example,
we present the results for NYSE-High datasets.
<p></p>
<p><img src="result-selcon.png" alt="placeholder" title="Large example image">
</p>

<h3 id=cnt>Contacts</h3>

If you have any questions/suggestions, please contact <a href = "mailto: durgas@cse.iitb.ac.in"> Durga S</a>, 
<a href = "mailto: rishabh.iyer@utdallas.edu">Rishabh Iyer</a>,
<a href = "mailto: ganesh@cse.iitb.ac.in"> Ganesh Ramakrishnan</a>
and <a href = "mailto: abir@cse.iitb.ac.in"> Abir De</a>.  
 
<br></br> 
We use the website template from <a href "https://github.com/poole/hyde"> https://github.com/poole/hyde </a> </p> 
  </div>
  
 
  
</div>

 
    </div>

  </body>
</html>

